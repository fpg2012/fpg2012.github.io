---
title: "使用Intel Arc A770进行AI画图"
date: "2023-09-03"
description: ""
category: ["dev"]
tag: ["intel", "AI画图", "stable diffusion", "显卡", "Arc A770"]
comment: true
layout: post
---

> 本文假设读者对Linux、PyTorch有一定的了解，对AI绘画的原理有一定了解。

## 前言

网上对[Intel Arc A770]()的风评不太好，评论基本上都说这张卡“性能不如同等级的N卡“、“驱动不完善”、“未来可期未来买”。但是综合考虑了一下我的需求，我还是决定无视这糟糕的风评，入手这张卡。这卡有几个优点：

1. 16GB显存（同级别的N卡基本上显存都没它大）

2. Linux下驱动支持良好（前提是内核版本6.2+，对比起来，N卡在Linux下用十分痛苦）

3. 没有矿

4. 价格已经初具性价比了，非公版现在只要2000出头，二手应该会更便宜

5. 性能可能确实一般，但是对我而言够用

6. 驱动已经逐渐完善了

7. AV1编码（虽然PR不支持，但是kdenlive支持）

随着Intel在去年发布了Intel Extensions For Pytorch（IPEX），用A770进行机器学习，也并非不可能。

故为了充分利用我手上的这张卡，我打算来试试AI绘图、AI音色替换、自己部署ChatGLM之类的玩法。但正如标题，本文只涉及AI绘图。

## 配置基本环境

我个人使用的环境如下：

```
                   -`                    nth233@lune 
                  .o+`                   ----------- 
                 `ooo/                   OS: Arch Linux x86_64 
                `+oooo:                  Kernel: 6.4.12-arch1-1 
               `+oooooo:                 Uptime: 32 mins 
               -+oooooo+:                Packages: 1503 (pacman), 16 (flatpak) 
             `/:-:++oooo+:               Shell: zsh 5.9 
            `/++++/+++++++:              Resolution: 2560x1440 
           `/++++++++++++++:             DE: GNOME 44.4 
          `/+++ooooooooooooo/`           WM: Mutter 
         ./ooosssso++osssssso+`          WM Theme: Adwaita 
        .oossssso-````/ossssss+`         Theme: Adwaita [GTK2/3] 
       -osssssso.      :ssssssso.        Icons: Adwaita [GTK2/3] 
      :osssssss/        osssso+++.       Terminal: kgx 
     /ossssssss/        +ssssooo/-       CPU: AMD Ryzen 5 5600G with Radeon Graphics (12) @ 3.900GHz 
   `/ossssso+/:-        -:/+osssso+-     GPU: AMD ATI Radeon Vega Series / Radeon Vega Mobile Series 
  `+sso+:-`                 `.-/+oso:    GPU: Intel DG2 [Arc A770] 
 `++:.                           `-/+/   Memory: 3503MiB / 15275MiB 
 .`                                 `/
```

其中有几个要点：

1. 用Linux或WSL2。Windows我没有尝试过，因此请参照官方文档。

2. **确保内核版本 >= 6.2**（目前Arch Linux的lts内核还只是6.1）

IPEX只支持Ubuntu 22.04，但是实测用Arch Linux应该也没问题。如果求稳，可以用Docker。建议第一次尝试使用Docker，比较省心。

> 确保内核版本足够高，这点特别重要！

安装intel oneapi相关的工具。

```
# pacman -S intel-compute-runtime-bin intel-oneapi-basekit intel_gpu_tools
```

`intel_gpu_tools`提供了`intel_gpu_top`这类工具，可以比较方便地查看显卡的占用情况。

似乎安装以上的包就已经足够了。但为了以防万一，确保读者可以复现，我这里列出我安装的所有和intel有关的包。

```
$ pacman -Qs intel

local/intel-compute-runtime-bin 23.22.26516.18-1
    Intel Graphics Compute Runtime for oneAPI Level Zero and OpenCL Driver (pre-compiled binaries)
local/intel-gpu-tools 1.27-2
    Tools for development and testing of the Intel DRM driver
local/intel-graphics-compiler-bin 1:1.0.14062.11-1
    Intel Graphics Compiler for OpenCL (pre-compiled binaries)
local/intel-media-driver 23.3.1-1
    Intel Media Driver for VAAPI — Broadwell+ iGPUs
local/intel-oneapi-basekit 2023.2.0.49397-1
    Intel oneAPI Base Toolkit for Linux
local/libmfx 23.2.2-2
    Intel Media SDK dispatcher library
local/libva-utils 2.19.0-1
    Intel VA-API Media Applications and Scripts for libva
local/onetbb 2021.9.0-1
    High level abstract threading library (oneAPI Threading Building Blocks)
local/openimagedenoise 1.4.3-1
    Intel(R) Open Image Denoise library
local/openpgl 0.5.0-5
    Intel Open Path Guiding Library
local/vulkan-intel 1:23.1.6-4
    Intel's Vulkan mesa driver
```

### 使用Docker

首先，安装Docker，并且启用docker服务。

```
# pacman -S docker
# sudo systemctl start docker
```

拉取Intel官方的docker映像（image）

```
# docker pull intel/intel-extension-for-pytorch:xpu-flex-2.0.110-xpu
```

运行映像。

```
# docker run --rm -it --privileged --device=/dev/dri --ipc=host <image_name>:<tag> bash
```

然后检查一下是否能跑。在docker里运行以下代码，看能否正确输出IPEX的版本、显卡设备的信息。

```
$ python -c "import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__); [print(f'[{i}]: {torch.xpu.get_device_properties(i)}') for i in range(torch.xpu.device_count())];"
```

到此为止，环境就配好了。

### 不使用Docker

确保安装了Python 3.11和对应的`python-pip`。

`cd`到一个合适的地方，然后创建Python虚拟环境，随后启用虚拟环境。

```
$ cd ~/workspace/intel_draw
$ python -m venv venv
$ source ./venv/bin/activate
```

到下载Python 3.11对应版本的PyTorch、torchvison和IPEX的whl文件。截至9月3日，是下面这三个。

```
torch-2.0.1a0+cxx11.abi-cp311-cp311-linux_x86_64.whl
torchvision-0.15.2a0+cxx11.abi-cp311-cp311-linux_x86_64.whl
intel_extension_for_pytorch-2.0.110+xpu-cp311-cp311-linux_x86_64.whl
```

然后一一使用`pip`装到虚拟环境中。

```
$ pip install torch-2.0.1a0+cxx11.abi-cp311-cp311-linux_x86_64.whl
$ pip install torchvision-0.15.2a0+cxx11.abi-cp311-cp311-linux_x86_64.whl
$ pip install intel_extension_for_pytorch-2.0.110+xpu-cp311-cp311-linux_x86_64.whl
```

然后，配置必要的环境变量。Intel提供了一个脚本，我这里是在`/opt/intel/oneapi`下面，执行这一脚本即可。之后每次使用IPEX前，都要先执行这一脚本。

```
$ source /opt/intel/oneapi/setvars.sh
```

然后运行以下代码，看能否正确输出IPEX的版本、显卡设备的信息。

```
$ python -c "import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__); [print(f'[{i}]: {torch.xpu.get_device_properties(i)}') for i in range(torch.xpu.device_count())];"
```

> 如果需要使用Python 3.10或者更老的版本，可以conda创建虚拟环境，同时环境时指定Python版本。
> 
> ```
> conda create -n intel_draw python==3.10.13
> ```
> 
> 启用这个刚刚创建的`intel_draw`环境，然后再从头开始配置环境。注意Python 3.10对应的`whl`文件和3.11不同，注意检查。

到此为止，基本的环境就配好了。

## 配置ComfyUI

Stable Diffusion WebUI虽然是最流行的WebUI，但它的没有直接支持Intel独显。虽有几个fork支持Intel独显，但是更新不如原项目勤快。所以我还是选择把intel独显支持整合进主分支的ComfyUI。

> 其实也可以不使用任何的WebUI，直接自己手动调用模型，可能还更简单一些。

把comfyui的仓库克隆到本地。这里是放在`~/workspace/ComfyUI`

```
$ git clone https://github.com/comfyanonymous/ComfyUI
```

然后启用刚才配置好的基本环境。

> 如果使用的是docker，启动容器的命令要添加几个选项。如下（注意根据自己的实际情况进行改动）：
> 
> ```
> # docker run --rm -it -v /home/nth233/workspace/ComfyUI:/opt/sd -p 8188:8188 --privileged --device=/dev/dri --ipc=host <image_name>:<tag> bash
> ```
> 
> `-v`是把我们刚刚克隆下来的ComfyUI的代码映射到docker容器中。`-p`是为了之后能够访问容器中运行的ComfyUI服务端，进行端口映射。

安装ComfyUI所需的基本依赖，在`~/workspace/ComfyUI`（如果使用docker，按照上面的命令启动，那么是`/opt/sd`）下，执行：

```
$ pip install -r requirements.txt
```

然后就可以启动ComfyUI了。

```
$ python main.py --listen 0.0.0.0 --use-split-cross-attention
```

然后可在浏览器中打开`localhost:8188`，即可看到ComfyUI。

## 下载模型

目前比较流行的是Stable Diffusion 1.5，因此我们以此为例。

从[huggingface](https://huggingface.co/runwayml/stable-diffusion-v1-5)上下载Stable Diffusion 1.5的checkpoint（如图）。下载完之后，放到`ComfyUI/models/checkpoints`下。

![Stable Diffusion 1.5 safetensors](/assets/img/post/sd-ckpt.png)

读者还可以根据需要，下载其他模型的checkpoint放进来选用。如果有需要，还可以上civitai上下载各种各样的LoRA模型，放到`lora`文件夹下面。ControlNet同理

然后，参考ComfyUI的文档和示例，就可以开始画图了。

## 结果

结果还行。但是似乎画图速度不算太快。但是不知是什么原因，intel这卡没法长宽太大的图，如果需要生成大图，可以考虑先生成小图，然后其他的模型超分辨率到高清大图。

<iframe src="//player.bilibili.com/player.html?aid=787658335&bvid=BV1214y1171Z&cid=1246931591&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 100%;aspect-ratio: 1920/1080;"> </iframe>

## 参考

[IPEX文档](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/installations/linux.html)

[I 卡也要炼！本地运行 Stable Diffusion & ComfyUI](https://kwaa.dev/stable-diffusion)

[ComfyUI](https://github.com/comfyanonymous/ComfyUI)
