---
layout: post
title: 概率论数理统计复习笔记
tag: [math, 概率论, 数理统计]
category: [note]
description: 陈年概率论笔记
date: 2020-10-01
comment: true
---

> 很久之前的概率论数理统计笔记，如今为了复习而放上来。也许会有大量错漏（就像我的其他blog一样），但我也懒得逐一校对纠正了。

## Part 1 概率论

### 随机事件和概率

> 随机事件才有概率
> 

#### 基本概念

1. 基本事件、样本点

2. 事件的关系（包含、互斥、相容、对立、相等、独立）

3. 事件的运算（和、积、差）

   > $A-B = A\overline B$

4. 概率的统计定义和公理化定义
   1. 统计定义：从频率出发，实验次数越多，频率越接近概率

   2. 公理化定义：设$E$是一次随机试验，那么对$E$的所有事件赋予一个实数，满足

      1. 非负性
      2. 归一性
      3. 可列可加性

      那么$P(A)$为概率

5. 概率运算的性质

   1. 对立事件和概率为1

   2. 包含关系蕴含着概率大小关系

   3. 概率加法公式：$P(A+B) = P(A) + P(B) - P(A \cap B)$

      > 可列个事件的加法公式

   4. 概率减法公式：$P(A-B) =P(A) - P(A\cap B)$

6. 条件概率：$P(B|A) = \frac{P(AB)}{P(A)}$，在$A$发生的条件下，$B$发生的概率

7. 完备事件组，样本空间的划分

8. 条件概率运算的性质：参考前面概率的性质，只是后面多加一个条件

9. **条件概率相关公式**

   1. 乘法公式：$P(AB) = P(A)P(B|A)$

      > 乘法公式意味着两个事件同时发生，与分别先后发成（如果对顺序没有要求）是等价的

   2. 全概率公式：$P(A) = \sum P(A|B_i)$，$B_1, B_2, \dots ,B_n$是一个划分

      > 全概率公式意味着一个事件有多种不同原因
      >
      > 也可以将事件的划分类比线性空间的一个正交基，全概率公式便是把一个事件的概率表示成多个事件的线性组合。

   3. Bayes公式：$P(B_i | A) = \frac{P(B_i)P(A|B_i)}{P(A)}$，当时左边是后验概率，$P(B_k)$为先验概率

      > 贝叶斯公式意味着一种从结果反推原因，获知已知一个结果已经发生，“更新”原因发生的概率。考虑酒桌上的摇色子游戏。考虑检测中的假阳性和假阴性（涉及到后面的假设检验）

10. 随机事件的独立性：$P(AB) = P(A)P(B)$

11. 两事件独立，则他们的对立事件也独立

12. n重Bernoulli实验

#### 例题

1. 拿枪问题（装信封问题）〔条件概率，减法公式（容斥原理）〕
2. 摸奖问题（抓阄问题）〔摸奖与顺序无关〕
3. 集合概型：等车问题〔画图解决〕

#### Pitfalls and Fallacies

1. $A -B = A\overline B$
2. 独立不等于对立
3. 互斥不等于对立
4. 两个事件同时发生，等价于依次发生且不考虑它们的顺序
5. 摸奖和顺序无关

### 随机变量和概率分布

#### 基本概念

1. 随机变量：$f: \Omega \to \mathbb R$，一个随机事件对应一个实数，那么这个实数便是随机变量

   > 这意味着随机变量是一个数，$f$是一种特殊的映射。对于一个数，研究它的性质就比较容易了。

2. 随机变量的分类

   1. 离散型
   2. 其他（包含连续型，也包含各种缝合怪，主要研究连续型）

   > 对于离散性，研究分布律
   >
   > 对于连续型，研究概率密度函数
   >
   > 两者都可以统一为分布函数

3. 分布律

4. 分布函数：$F(x) = P\{X \le x\}$，二维联合分布：$F(x,y) = P\{X\le x, Y \le y\}$

   > 对于离散性，$F(x)$有很多间断点，间断点的跃度表示概率
   >
   > 对于连续型，$F(x)$可导，导函数为分布函数
   
5. 分布函数的性质

    1. 非负
    2. 右连续（**典型区间**决定的）
    3. 递增
    4. 归一

   

6. 概率密度函数的性质

7. 边缘分布：消灭掉一个维度的随机性（在实数上积分）

8. 条件分布

   $$
   P\{X = x_i | Y = y_i\} = \frac{P\{X = x_i, Y = y_i\}}{P\{Y = y_i\}} = \frac{p_{ij}}{p_{\cdot j}}
   $$

   $$
   \lim_{\epsilon \to 0}P\{X \le x |y - \epsilon \lt Y \le y + \epsilon\} = F_{X|Y}(x|y)
   $$

   可证
   
   $$
   f_{X|Y} = \frac{f(x,y)}{f_Y(y)}
   $$

   > 〔不考，但是有趣〕

9. 随机变量的独立：$F(x,y) = F_X(x)F_Y(y)$，$f(x,y) = f_X(x)f_Y(y)$

   > n维随机变量相互独立，定理与二维雷同。不过两两独立不意味着相互独立。

#### 常见分布

1. 一维

    1. 两点分布

    2. 二项分布：$X\sim B(n, p)$

    3. Poisson分布：$P\{X = k\} = \frac{\lambda^k}{k!}e^{-\lambda}$

      > $e^{-\lambda}$为归一化因子，有了它才能归一
      >
      > $X \sim \pi(\lambda)$
      >
      > ［泊松定理］泊松分布和二项分布的关系
      >
      > 若$np_n \to \lambda$，且$n \to \infty$，那么有$\displaystyle\lim_{n\to\infty} C_n^kp^k_nq^{n-k}_n = \frac{\lambda^k}{k!}e^{-\lambda}$
      >
      > 具体实际操作中，$n \ge 10$，且$p \le 0.1$，那么可以用泊松分布计算二项分布
      >
      > ［实际背景］相同间隔内到达的乘客批数，电话总机某段事件内接收到的呼叫次数。（大概就是与连续时间挂钩的“二项分布”，因为每个时刻发生的「概率」相同）

    4. 几何分布：$P\{X = n\} = p^n$

      > ［实际背景］过红绿灯，在第几个红绿灯前停下。

    5. 均匀分布：$X\sim U(a, b)$

    6. 指数分布：$X\sim e(\theta)$，$\displaystyle f(x) = \begin{cases}\frac{1}\theta e^{-\frac{x}{\theta}} & x\ge 0 \\ 0\end{cases}$

      > ［实际背景］寿命

    7. **正态分布**：$X\sim N(\mu, \sigma^2)$

      > ［实际背景］最广泛的分布，比如测量误差
      >
      > 表达式要记：
      >
      > $$
      > f(x) = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
      > $$
      >
      > 图形要记，归一化的证明方法要记（积分前先平方，然后化成二元函数）
      >
      > ［归一化］$\frac{X-\mu}{\sigma}$
   
2. 二维

    1. 均匀

    2. 正态：$X\sim N(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho)$，其中$\rho$为相关系数。两个维度独立/不相关时，$\rho = 0$.

      > 表达式：如果$X \sim N(0,0,1,1,\rho)$
      >
      > $$
      > f(x,y) = \frac{1}{2\pi\sqrt{1-\rho^2}}e^{-\frac{1}{1-\rho^2}[x^2 + y^2 - 2\rho xy]}
      > $$
      >
      > 验证归一，请配方

3. 随机变量函数的概率

    > 谁是谁的谁
    > 找支撑

    1. 离散型：分布律重写
    
    2. 连续型
    
        1. 图像法
        
        2. 公式法（要求$h(x)$为$g(y)$的反函数
        
        $$
        f_Y(y) = \begin{cases} f_X(h(y)|h'(y)| & \alpha \lt y \lt \beta \\ 0 \end{cases}
        $$
    
        3. 分布函数法
      
    3. 二维
       
        > 如何推导？紧扣定义，找支撑。最后积分、利用独立性化简
       
        1. $Z = X + Y$：卷积公式
        
        $$
         \begin{aligned} 
         F_Z(z) = \iint_{x+y \le z} f(x,y)dxdy & =\int_{-\infty}^{+\infty}dy\int_{-\infty}^{z-y}f(x,y)dx \\
         & = \int_R\int^z_{-\infty}f(u-y,y)dudy
         \end{aligned}
        $$
        
         类似可得概率密度函数的公式。如果$X$、$Y$相互独立，最后可以推得卷积公式
         
        $$
         f_X * f_Y = \int_Rf_X(x)f_Y(z-x)dx
        $$
      
        2. $Z = \max(X,Y)$
        
            很简单，就是分布函数相同
        
        3. $Z = \min(X,Y)$
        
            分布函数分别被1减，然后相乘再被1减
        
        4. $Z = XY$
        
        5. $Z = X/Y$


#### 例题

1. 给概率密度函数求分布函数
2. 判断独立性
3. 求边缘
4. 已知独立性求参数

#### Pitfalls and Fallacies

1. 已知分布函数求概率，无论一维二维都需要割，很麻烦
2. 已知边缘不能反求联合，但是如果补充上条件，便可以得到联合
3. $\int_0^{\infty}e^{-x^2}dx = \frac{\sqrt{\pi}}{2}$

### 随机变量的数值特征

> 本章只有一个东西：数学期望

#### 基本概念

1. 数学期望

    1. 离散型：级数$\sum |x_kp_k|$收敛，那么定义$\sum x_kp_k$为$E(X)$
    2. 连续型：$\int |xf_X(x)|dx$收敛，那么定义$\int xf_X(x)dx$为$E(X)$

    > 绝对收敛是为了保证求和不受求和次序的影响

2. 随机变量函数的数学期望

    $Y = g(X)$

    1. 离散型：$E(Y) = E(g(X)) = \sum g(x_k)p_k$
    2. 连续型：$E(Y) = \int g(x)f(x)dx$
    3. 二维：$Z = g(X, Y)$，$E(Z) = \iint_{R^2}g(x, y)f(x,y)dxdy$

3. **数学期望的性质**

    1. $E(C) = C$
    2. 完美的线性：$E(aX+bY) = aE(X) + bE(Y)$
    3. $E(XY) = E(X)E(Y)$，当且仅当二者不相关时（联系协方差）

4. 矩

    1. $k$阶原点矩：$E(X^k)$
    2. $k$阶中心矩：$E((X-\bar X)^k)$

5. 方差：二阶中心矩，$D(X)$，或$\sigma^2$

6. 方差的性质

    1. $D(C) = 0$
    2. $D(X\pm C) = D(X)$
    3. $D(CX) = C^2D(X)$
    4. $D(X_1 + X_2) = D(X_1) + D(X_2)$，当且仅当二者不相关
    5. 与期望：$D(X) = E(X^2) - E(X)^2$

    > 第五点根据定义可以证明。

7. 协方差：$\mathrm{Cov}(X,Y) = E[(X-\bar X)E(Y - \bar Y)]$

8. 协方差的性质

    1. 交换
    2. 双线性
    3. 与期望的关系：$E(XY) - E(X)E(Y) = \mathrm{Cov}(X, Y)$
    4. 与方差的关系：$D(X \pm Y) = D(X) + D(Y) \pm 2\mathrm{Cov}(X,Y)$

9. 相关系数：标准化的协方差：$\rho_{XY} = \frac{\mathrm{Cov}(X,Y)}{\sqrt{D(X)D(Y)}}$

10. 相关系数与线性相关：$|\rho_{XY}| = 1 \Leftrightarrow P\{Y = a + bX\} = 1$

    > 相关系数仅仅衡量线性关系，不代表两个变量没有其他关系

11. 协方差矩阵、多维正态：略

#### 常见分布的数学期望和方差

1. 均匀分布：$(a+b)/2$，$(b-a)^2/12$
2. 指数分布：$\theta$，$\theta^2$
3. 正态分布：$\mu$，$\sigma^2$
4. 二项分布：$np$、$np(1-p)$
5. 泊松分布：$\lambda$，$\lambda$

### 大数定律和中心极限定理

#### 基本概念

1. 大数定理
    1. Chebyshev不等式［误差的概率有上限］

        条件：已知期望和方差

        $P\{|X - \mu| \ge \epsilon\} \le \sigma^2/\epsilon^2$

        $P\{|X-\mu| \le \epsilon\} \ge 1 - \sigma^2/\epsilon^2$

    2. 「依概率收敛」／「服从大数定理」：$\lim_{n\to\infty}P\{|\bar X_n - a_n| \ge \epsilon\} \le \sigma^2/\epsilon$$
    3. Chebyshev大数定理的特殊情况

        条件：随机变量序列独立，方差有奇性（方差已知且相等）

        这个随机变量序列按概率收敛于期望

        > 由此可以得到两个结论：连续函数保收敛。平均值趋近于期望

    4. Bernoulli大数定理：针对二项分布，频率稳定性

        条件：n次简单重复试验，$n_A$为$A$发生的次数，$p$为$A$发生的概率

        $\left|\frac{n_A}{n} - p\right|$依概率收敛于$1$

        > 使用Chebyshev大数定理证明

    5. Хинчин大数定理

        条件：随机变量序列，独立同分布

        结论和Chebyshev大数定理一样

2. 中心极限定理

    1. Lindbeyg-Levy中心极限定理

        对象：随机变量序列

        条件：独立同分布，期望方差已知
        
        $$
        Y = \frac{\sum X_k - E(\sum X_k)}{\sqrt{D(\sum X_k)}} = \frac{\sum X_k - n\mu}{\sqrt{n}\sigma}
        $$
        
        **和**的标准化随机变量按概率收敛于**标准正态**

    2. de Moivre-Laplace中心极限定理

        对象：二项分布总体
        
        $$
        Y = \frac{X-np}{\sqrt{np(1-p)}}
        $$
        
        趋近于标准正态。

        > 二项分布的极限的标准正态

## Part 2 数理统计

> 五个统计量、四大分布、六大定理

### 基本概念

1. 总体：类，一个随机变量

2. 样本：对象，一次试验的结果，一个随机变量

3. 简单随机样本、简单随机抽样

    1. 代表性：同分布
    2. 独立性：所有样本相互独立

4. 统计量：样本不含参的函数

5. **五个统计量**

    1. 样本均值
    2. **样本方差**：$\frac{n}{n-1}B_2$
    3. 样本标准差
    4. $A_k$，k阶原点矩
    5. $B_k$，k阶中心矩

    > 为何方差这么奇葩？因为他是无偏估计

### 以正态为基础，统计学三大分布

> 龙珑的四大分布

1. 正态分布：后面所有分布的基础

2. $\chi^2$分布：样本平方和的分布

    1. 定义：$\chi^2 = \sum X_i^2$，$\chi^2 \sim \chi^2(n)$，自由度为$n$的$\chi^2$分布

    2. 性质

        1. 期望、方差：$E(\chi^2) = n$，$D(\chi^2) = 2n$

            > 证明出奇的简单，利用独立性！

        2. $\alpha$分位点［一般是上分位点］

        3. 图形不对称

        4. Fisher的定理：$n$够大的时候，$\chi^2_\alpha(n) = \frac{1}{2}(z_\alpha + \sqrt{2n-1})^2$

3. $t$分布

    1. 定义：$T = \frac{X}{\sqrt{\frac{Y}{n}}}$，$T\sim t(n)$（$X$标准正态，$Y$服从$\chi^2$）

        > 标准正态除以一个「等价的」标准正态

    2. 性质

        1. 图形和标准正态相似
        2. 偶函数
        3. $n>30$时，近似标准正态

4. $F$分布

    1. 定义：$F = \frac{U/m}{V/n}$，$U,V$都服从$\chi^2$
    2. 性质
        1. $\frac{1}{F(m,n)} \Leftrightarrow F(n, m)$
        2. $F_{1-\alpha}(n,m) = \frac{1}{F_\alpha(n,m)}$
        3. 图形不对称，和$\chi^2$分布有点像

#### 六大定理

> 分为对**单个**正态总体、和**两个**正态总体均值和方差的估计。又能分成已知方差、未知方差
>
> 均值做差，方差做比

1. 定理一：已知$\sigma^2$对$\mu$的估计

    1. $\bar X \sim N(\mu, \frac{\sigma^2}{n})$
    2. $\frac{\bar X - \mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0,1)$

    > 2是1的标准化。这个定理意味不仅意味着均值趋于方差，而且意味着方差

2. 定理二：未知$\sigma$对$\mu$估计

    $\frac{\bar X - \mu}{\frac{S}{\sqrt{n}}} \sim t(n)$

    > 把$\sigma$换成$S$

3. 定理三：对$\sigma$估计

    $$
    \frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)
    $$

    > 所有的$n-1$都来自这个定理

4. 定理四：$\bar{X}$和$S$独立

    > 使用增量法理解

5. 定理五：两个正态总体均值差的关系

    1. 已知$\sigma^2$

        $$
        \frac{(\bar X - \bar Y) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n^2} + \frac{\sigma_2^2}{m^2}}} \sim N(0, 1)
        $$

    2. 未知$\sigma^2$。把分母换成$S_w(\sqrt{\frac{1}{n_1}+\frac{1}{n_2}})$，服从$t(n + m - 2)$分布

        > $S_w^2 = \frac{(m-1)S_x^2 + (n-1)S_2^2}{(m-1) + (n-1)}$，两者方差的「均值」

6. 定理六：两个正态总体方差比的关系

    $$
    \frac{\frac{S_X^2}{S_Y^2}}{\frac{\sigma_1^2}{\sigma_2^2}}\sim F(m-1, n-1)
    $$

7. 

### 参数估计

#### 概论

1. 如何估计：点估计、区间估计
2. 如何评价：三个评价标准

#### 点估计

##### 矩估计

1. 估计原理：辛钦大数定理，$\bar X^k \overset{P}{\to} E(X^k)$

2. 方法：列方程、解方程，列到方程足够为止，解出来的参数就是估计量。

    $$
    \mu_k(\hat\theta_1, \dots , \hat\theta_m) = \bar X_m
    $$

3. 优点：简单

4. 缺点

    1. 不唯一
    2. 有时不合理（考虑均匀分布）

##### 极大似然估计

> 首选

1. 估计原理：最大似然原理（与小概率原理对立）

2. 方法

    构建似然函数，然后求极大值点。

    似然函数有两种情况

    1. 离散情形下
    
        $$
        L(\theta) = \prod^n_{i=1}P(x_i;\theta)
        $$

    2. 连续情形下
    
        $$
        L(\theta) = \prod^n_{i=1}f(x_i, \theta)
        $$

    > 由于出现累乘，常常需要对$L(\theta)$取对数。不过$\ln$不改变极大值点，因此很方便。

##### 评价

1. 无偏：$E(\hat\theta) = \theta$，基本要求

2. 有效：两个**无偏估计**，方差小的更有效

    > 无偏是有效的前提，都不无偏，那就一定不有效

3. 相合：$\hat\theta \overset{P}{\to} \theta$

    > 无偏和有效是相合的前提

#### 区间估计

> 单侧估计和双侧估计
>
> 都得往正态上靠
>
> 联系定理

1. 置信区间：$\theta$是要估计的未知参数，$\hat\theta_1$、$\hat\theta_2$是两个**统计量**，如果

    $$
    P\{\hat\theta_1 \lt \theta \lt \hat\theta_2 \} = 1-\alpha
    $$
    
    那么随机区间$(\hat\theta_1, \hat\theta_2)$是**置信度**为$1-\alpha$的置信区间，$\alpha$叫**显著性水平**

2. 方法

    1. 正态总体
        1. 双侧
            1. 已知$\sigma$估计$\mu$：$\left(\bar X \pm \frac{\sigma}{\sqrt{n}}z_\frac{\alpha}{2}\right)$，定理分母乘以分位点
            2. 未知$\sigma$也是一样的，不过分布改成t分布
            3. 估计$\sigma^2$，由于估计量放在分母上，所以要注意$1-\alpha/2$和$\alpha/2$的位置关系。定理分子除以分位点。
        2. 单侧：双侧改一改
    2. 非正态：中心极限定理

### 假设检验

1. 基本思想

    1. 证伪
    2. 样本说服力不同
    3. 「差异显著」=「小概率事件发生」几乎是不可能的。如果「差异显著」，就成功证伪

2. 显著性水平：小概率$\alpha$

3. 检验统计量：要检验的量

4. 原假设：$H_0$，默认发生

5. 备择假设：$H_1$，默认不发生

    > $H_0$一般认为是大概率的，$H_1$一般认为是小概率的。同一个问题，假设放置的位置不同，体现着立场不同。
    >
    > 比如：做有罪推定，还是无罪推定？没有证据，是否认为犯罪？假定无恶意，还是假定恶意？
    >
    > 但是这其中并不只是概率的问题，还涉及到价值观、目的。比如，病毒检测，我们不应该放过任何一个潜在的携带者，$H_1$应该放阳性。

    > 研发中，刚刚想出的新方法一般不优于旧方法。
    >
    > $H_0$和$H_1$未必对立

6. 拒绝域：小概率发生的区间，落入拒绝域，则说明「差异显著」，成功证伪。

7. 假设检验可能发生的错误

    > 有两个概率需要控制，一个是冤枉好人的概率，一个是放走坏人的概率。减小一个，就会增大另一个，那么我们应该选择控制哪一个呢？

    1. 弃真概率（第一类错误发生的概率）
    
        $$
        \alpha = P\{\text{拒绝$H_0$}|\text{$H_0$为真}\}
        $$
        
        即为显著性水平。显著性检验只控制弃真概率。

        > 也就是，控制冤枉好人的概率。

    2. 取伪概率（第二类错误发生的概率）
    
        $$
        \beta = P\{\text{接受$H_0$} | \text{$H_0$为假}\}
        $$
        
        检验功效$1-\beta$

        > 考虑病毒检测，如果一个检测方式，如果检测得到阳性，那么大概率是有病，那么这种检测功效是不错的。（即便可能存在一大批假阴性）。

8. 双侧检验：$H_1: ??? \ne ???$

9. 单边检验

10. 与置信区间的关系
